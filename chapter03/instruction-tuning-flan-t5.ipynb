{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff4c5e9-fb09-45c4-a7b4-bad890f5e198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "templates = [\n",
    "        (\"Please answer this question: {question}\", \"{answer}\"),\n",
    "        (\"{question}\", \"{answer}\"),\n",
    "        (\"Write the answer: {question}\", \"{answer}\"),\n",
    "        (\"What is the answer: {question}\", \"{answer}\"),\n",
    "        (\"Answer this question.\\n\\n{question}\", \"{answer}\"),\n",
    "        (\"Answer the following question. {question}\", \"{answer}\"),\n",
    "        (\"Question: {question}\\nAnswer:\", \"{answer}\"),\n",
    "        (\"{question}???\", \"{answer}\"),\n",
    "        (\"Trivia question: {question}\\nAnd the answer is?\", \"{answer}\"),\n",
    "        (\"{question}\\nWhat is the answer?\", \"{answer}\"),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d17af79c-0820-49bf-b0ad-f0947c4181c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Instruction #1\n",
      "Please answer this question: {question}\n",
      "{answer}\n",
      "\n",
      "# Instruction #2\n",
      "{question}\n",
      "{answer}\n",
      "\n",
      "# Instruction #3\n",
      "Write the answer: {question}\n",
      "{answer}\n",
      "\n",
      "# Instruction #4\n",
      "What is the answer: {question}\n",
      "{answer}\n",
      "\n",
      "# Instruction #5\n",
      "Answer this question.\n",
      "\n",
      "{question}\n",
      "{answer}\n",
      "\n",
      "# Instruction #6\n",
      "Answer the following question. {question}\n",
      "{answer}\n",
      "\n",
      "# Instruction #7\n",
      "Question: {question}\n",
      "Answer:\n",
      "{answer}\n",
      "\n",
      "# Instruction #8\n",
      "{question}???\n",
      "{answer}\n",
      "\n",
      "# Instruction #9\n",
      "Trivia question: {question}\n",
      "And the answer is?\n",
      "{answer}\n",
      "\n",
      "# Instruction #10\n",
      "{question}\n",
      "What is the answer?\n",
      "{answer}\n"
     ]
    }
   ],
   "source": [
    "for idx, template in enumerate(templates):\n",
    "    print(f\"\\n# Instruction #{idx+1}\")\n",
    "    print('\\n'.join(template)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b918f252-7025-4945-a22e-961a43f5bae3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the custom dataset\n",
    "dataset = load_dataset(\"yahoo_answers_qa\")\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Please answer this question: \n",
    "\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\n",
    "{answer}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "831dfe0a-6a3e-4655-b6b6-2236e4c04e53",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'answer', 'nbestanswers', 'main_category'],\n",
       "        num_rows: 87362\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3f658da-e3b4-44f6-af54-64488628bafb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 87362/87362 [00:08<00:00, 10144.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def _row_to_instruction(row):\n",
    "    row['instruction'] = prompt_template.format(question=row[\"question\"], answer=row[\"answer\"])\n",
    "    return row\n",
    "instruction_dataset = dataset.map(_row_to_instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f8dbdf6-61fb-4875-8909-726d0c7fa8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please answer this question: \n",
      "\n",
      "How to get rid of a beehive?\n",
      "\n",
      "Answer:\n",
      "\n",
      "Call an area apiarist.  They should be able to help you and would most likely remove them at no charge in exchange for the hive.  The bees have value and they now belong to you.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(instruction_dataset['train'][1]['instruction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "252ba260-ca46-42a3-8833-ef59b896b9cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://us-kfp.pkg.dev/ml-pipeline/google-cloud-registry/t5-finetuning/sha256:4bce8d9de89913a5d97ab803e8f0941c916a054a50539a749b00f3f0b0179a83'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PIPELINE_TEMPLATE_FILE = ! dirname $(gcloud artifacts files list \\\n",
    "  --repository=google-cloud-registry \\\n",
    "  --project=ml-pipeline \\\n",
    "  --location=us \\\n",
    "  --package=t5-finetuning \\\n",
    "  --sort-by=~UPDATE_TIME \\\n",
    "  --format=\"value(FILE)\" \\\n",
    "  --limit=1)\n",
    "PIPELINE_TEMPLATE_FILE = PIPELINE_TEMPLATE_FILE[0]\n",
    "PIPELINE_TEMPLATE_URI = f\"https://us-kfp.pkg.dev/ml-pipeline/google-cloud-registry/{PIPELINE_TEMPLATE_FILE}\"\n",
    "PIPELINE_TEMPLATE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9e494-9652-4a64-90bb-5c03f60661ff",
   "metadata": {},
   "source": [
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pipeline_templates_t5x.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba80d2a-f520-4d5e-89e2-0965b5383cf6",
   "metadata": {},
   "source": [
    "- Preparing dataset\n",
    "    - [ ] Download dataset\n",
    "    - [ ] Add instructions\n",
    "    - [ ] Split to train and test\n",
    "    - [ ] Convert dataset to TFRecords\n",
    "\n",
    "- Tuning\n",
    "    - [ ] 1. Model Garden & Pipeline Run\n",
    "    - [ ] 2. Vertex AI Pipeline SDK\n",
    "    - [ ] 3. Vertex Training Custom Job with HuggingFace \n",
    "    \n",
    "- Evaluation\n",
    "    - [ ] Prepare evaluation dataset\n",
    "    - [ ] Define metrics\n",
    "    - [ ] Run evaluation\n",
    "    - [ ] Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15194dd6-1b18-41c1-8136-09ed532d4585",
   "metadata": {},
   "source": [
    "## Bring your own tuning script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a37119-442b-4c4d-a040-0ae6f3c9381e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install nltk\n",
    "pip install datasets\n",
    "pip install transformers[torch]\n",
    "pip install tokenizers\n",
    "pip install evaluate\n",
    "pip install rouge_score\n",
    "pip install sentencepiece\n",
    "pip install huggingface_hub\n",
    "pip install evaluate[evaluator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1702258-09e2-4b3d-a2f3-6178a4f96ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "   preds, labels = eval_preds\n",
    "\n",
    "   # decode preds and labels\n",
    "   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "   decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "   # rougeLSum expects newline after each sentence\n",
    "   decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "   decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "   result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "  \n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c01ae18-4b57-47a3-8652-38bfcfca1b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (T5ForConditionalGeneration, \n",
    "                          Seq2SeqTrainingArguments, Seq2SeqTrainer,\n",
    "                          T5Tokenizer, DataCollatorForSeq2Seq)\n",
    "\n",
    "# Global Parameters\n",
    "L_RATE = 3e-4\n",
    "BATCH_SIZE = 8\n",
    "PER_DEVICE_EVAL_BATCH = 4\n",
    "WEIGHT_DECAY = 0.01\n",
    "SAVE_TOTAL_LIM = 3\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# Acquire the training data from Hugging Face\n",
    "DATASET_NAME = \"yahoo_answers_qa\"\n",
    "dataset = load_dataset(DATASET_NAME)\n",
    "dataset = yahoo_answers_qa[\"train\"].train_test_split(test_size=0.3)\n",
    "\n",
    "# Load the tokenizer, model, and data collator\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenize text to tokens\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "prompt_template = \"\"\"Please answer this question: \\n\\n{question}\\n\\nAnswer:\"\"\"\n",
    "\n",
    "def preprocess(rows):\n",
    "    \"\"\"Add instructions via prompt template, tokenize the text, and set the labels\"\"\"\n",
    "    inputs = [prompt_template.format(question=q) for q in rows[\"question\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "    # The \"labels\" are the tokenized outputs:\n",
    "    labels = tokenizer(text_target=rows[\"answer\"], \n",
    "                       max_length=512,\n",
    "                       truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# Define data collator to pad inputs and labels\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "   output_dir=\"./results\",\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   learning_rate=L_RATE,\n",
    "   per_device_train_batch_size=BATCH_SIZE,\n",
    "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "   weight_decay=WEIGHT_DECAY,\n",
    "   save_total_limit=SAVE_TOTAL_LIM,\n",
    "   num_train_epochs=NUM_EPOCHS,\n",
    "   predict_with_generate=True,\n",
    "   push_to_hub=False\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_dataset[\"train\"],\n",
    "   eval_dataset=tokenized_dataset[\"test\"],\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f87bef2-2e81-4744-90a2-4d7b557232cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef9f90b-cecb-4f4f-a298-d18e1b9916c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer, model, and data collator\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d985a-e61f-4772-abc7-e9a77bf36b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquire the training data from Hugging Face\n",
    "DATA_NAME = \"yahoo_answers_qa\"\n",
    "yahoo_answers_qa = load_dataset(DATA_NAME)\n",
    "yahoo_answers_qa = yahoo_answers_qa[\"train\"].train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60967200-15c6-458f-9510-f816145a2e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11762aac-a4ca-4a6e-8e90-bb93d94a0bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the preprocessing function across our dataset\n",
    "tokenized_dataset = yahoo_answers_qa.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe37b84-d4cc-45e8-afd1-4110ad2bd80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"punkt\", quiet=True)\n",
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae48416-9730-4be1-8975-1640f54793f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "   preds, labels = eval_preds\n",
    "\n",
    "   # decode preds and labels\n",
    "   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "   decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "   # rougeLSum expects newline after each sentence\n",
    "   decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "   decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "   result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "  \n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2747cd5-b7a0-4e3c-96bd-0e626bf67415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "L_RATE = 3e-4\n",
    "BATCH_SIZE = 8\n",
    "PER_DEVICE_EVAL_BATCH = 4\n",
    "WEIGHT_DECAY = 0.01\n",
    "SAVE_TOTAL_LIM = 3\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "   output_dir=\"./results\",\n",
    "   evaluation_strategy=\"epoch\",\n",
    "   learning_rate=L_RATE,\n",
    "   per_device_train_batch_size=BATCH_SIZE,\n",
    "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
    "   weight_decay=WEIGHT_DECAY,\n",
    "   save_total_limit=SAVE_TOTAL_LIM,\n",
    "   num_train_epochs=NUM_EPOCHS,\n",
    "   predict_with_generate=True,\n",
    "   push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac90ffe0-1436-4faf-b72a-85dc8fd8a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_dataset[\"train\"],\n",
    "   eval_dataset=tokenized_dataset[\"test\"],\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ff257a-9ff3-4d8e-a0fd-0742675b8f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2e401-e534-42e7-a3b3-0e0c38963cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Vertex AI custom job with container image spec\n",
    "job = aiplatform.CustomContainerTrainingJob(display_name=JOB_NAME,\n",
    "                                            container_uri=TRAIN_IMAGE_URI)\n",
    "\n",
    "# Submit the custom job to Vertex AI training service\n",
    "model = job.run(replica_count=1,\n",
    "                machine_type=\"n1-standard-8\",\n",
    "                accelerator_type=\"NVIDIA_TESLA_V100\",\n",
    "                accelerator_count=1,\n",
    "                sync=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f75b13b-efc5-4319-90aa-9ad97b7374a2",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6614a533-366c-4cbe-8dda-f5d0ec5a2245",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/genai-gcp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "DATA_NAME = \"yahoo_answers_qa\"\n",
    "dataset = load_dataset(DATA_NAME)\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e066ed74-a849-41bd-a3fb-c559c6d9d43b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 26209/26209 [00:03<00:00, 7276.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def _row_to_instruction(row):\n",
    "    row['context'] = prompt_template.format(question=row[\"question\"])\n",
    "    return row\n",
    "prompt_template = \"\"\"Please answer this question: \\n\\n{question}\\n\\nAnswer:\"\"\"\n",
    "eval_dataset = dataset[\"test\"].map(_row_to_instruction)\n",
    "eval_dataset = eval_dataset.remove_columns([\"question\", \"main_category\", \"nbestanswers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f8b7c3f-0613-45a9-9778-22b6d90e9e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'answer', 'context'],\n",
       "    num_rows: 26209\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "007266b2-f851-4749-84c8-58b0e23e0cf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from evaluate import evaluator\n",
    "task_evaluator = evaluator(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a0bb6eb-1a99-4f41-acd6-679ec983faa7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtask_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel_or_pipeline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PreTrainedModel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TFPreTrainedModel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msubset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmetric\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvaluationModule\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PreTrainedTokenizer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstrategy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'simple'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bootstrap'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'simple'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mconfidence_level\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_resamples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m9999\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrandom_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquestion_column\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'question'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcontext_column\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'context'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mid_column\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlabel_column\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'answers'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msquad_v2_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Compute the metric for a given pipeline and dataset combination.\n",
       "Args:\n",
       "    model_or_pipeline (`str` or `Pipeline` or `Callable` or `PreTrainedModel` or `TFPreTrainedModel`, defaults to `None`):\n",
       "        If the argument in not specified, we initialize the default pipeline for the task (in this case\n",
       "        `text-classification` or its alias - `sentiment-analysis`). If the argument is of the type `str` or\n",
       "        is a model instance, we use it to initialize a new `Pipeline` with the given model. Otherwise we assume the\n",
       "        argument specifies a pre-initialized pipeline.\n",
       "    data (`str` or `Dataset`, defaults to `None`):\n",
       "        Specifies the dataset we will run evaluation on. If it is of type `str`, we treat it as the dataset\n",
       "        name, and load it. Otherwise we assume it represents a pre-loaded dataset.\n",
       "    subset (`str`, defaults to `None`):\n",
       "        Defines which dataset subset to load. If `None` is passed the default subset is loaded.\n",
       "    split (`str`, defaults to `None`):\n",
       "        Defines which dataset split to load. If `None` is passed, infers based on the `choose_split` function.\n",
       "    metric (`str` or `EvaluationModule`, defaults to `None`):\n",
       "        Specifies the metric we use in evaluator. If it is of type `str`, we treat it as the metric name, and\n",
       "        load it. Otherwise we assume it represents a pre-loaded metric.\n",
       "    tokenizer (`str` or `PreTrainedTokenizer`, *optional*, defaults to `None`):\n",
       "        Argument can be used to overwrite a default tokenizer if `model_or_pipeline` represents a model for\n",
       "        which we build a pipeline. If `model_or_pipeline` is `None` or a pre-initialized pipeline, we ignore\n",
       "        this argument.\n",
       "    strategy (`Literal[\"simple\", \"bootstrap\"]`, defaults to \"simple\"):\n",
       "        specifies the evaluation strategy. Possible values are:\n",
       "        - `\"simple\"` - we evaluate the metric and return the scores.\n",
       "        - `\"bootstrap\"` - on top of computing the metric scores, we calculate the confidence interval for each\n",
       "        of the returned metric keys, using `scipy`'s `bootstrap` method\n",
       "        https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html.\n",
       "    confidence_level (`float`, defaults to `0.95`):\n",
       "        The `confidence_level` value passed to `bootstrap` if `\"bootstrap\"` strategy is chosen.\n",
       "    n_resamples (`int`, defaults to `9999`):\n",
       "        The `n_resamples` value passed to `bootstrap` if `\"bootstrap\"` strategy is chosen.\n",
       "    device (`int`, defaults to `None`):\n",
       "        Device ordinal for CPU/GPU support of the pipeline. Setting this to -1 will leverage CPU, a positive\n",
       "        integer will run the model on the associated CUDA device ID. If `None` is provided it will be inferred and\n",
       "        CUDA:0 used if available, CPU otherwise.\n",
       "    random_state (`int`, *optional*, defaults to `None`):\n",
       "        The `random_state` value passed to `bootstrap` if `\"bootstrap\"` strategy is chosen. Useful for\n",
       "        debugging.\n",
       "\n",
       "\n",
       "\n",
       "    question_column (`str`, defaults to `\"question\"`):\n",
       "        The name of the column containing the question in the dataset specified by `data`.\n",
       "    context_column (`str`, defaults to `\"context\"`):\n",
       "        The name of the column containing the context in the dataset specified by `data`.\n",
       "    id_column (`str`, defaults to `\"id\"`):\n",
       "        The name of the column containing the identification field of the question and answer pair in the\n",
       "        dataset specified by `data`.\n",
       "    label_column (`str`, defaults to `\"answers\"`):\n",
       "        The name of the column containing the answers in the dataset specified by `data`.\n",
       "    squad_v2_format (`bool`, *optional*, defaults to `None`):\n",
       "        Whether the dataset follows the format of squad_v2 dataset. This is the case when the provided dataset\n",
       "        has questions where the answer is not in the context, more specifically when are answers as\n",
       "        `{\"text\": [], \"answer_start\": []}` in the answer column. If all questions have at least one answer, this parameter\n",
       "        should be set to `False`. If this parameter is not provided, the format will be automatically inferred.\n",
       "    \n",
       "\n",
       "\n",
       "Return:\n",
       "    A `Dict`. The keys represent metric keys calculated for the `metric` spefied in function arguments. For the\n",
       "    `\"simple\"` strategy, the value is the metric score. For the `\"bootstrap\"` strategy, the value is a `Dict`\n",
       "    containing the score, the confidence interval and the standard error calculated for each metric key.\n",
       "\n",
       "Examples:\n",
       "```python\n",
       ">>> from evaluate import evaluator\n",
       ">>> from datasets import load_dataset\n",
       ">>> task_evaluator = evaluator(\"question-answering\")\n",
       ">>> data = load_dataset(\"squad\", split=\"validation[:2]\")\n",
       ">>> results = task_evaluator.compute(\n",
       ">>>     model_or_pipeline=\"sshleifer/tiny-distilbert-base-cased-distilled-squad\",\n",
       ">>>     data=data,\n",
       ">>>     metric=\"squad\",\n",
       ">>> )\n",
       "```\n",
       "\n",
       "<Tip>\n",
       "\n",
       "Datasets where the answer may be missing in the context are supported, for example SQuAD v2 dataset. In this case, it is safer to pass `squad_v2_format=True` to\n",
       "the compute() call.\n",
       "\n",
       "</Tip>\n",
       "\n",
       "```python\n",
       ">>> from evaluate import evaluator\n",
       ">>> from datasets import load_dataset\n",
       ">>> task_evaluator = evaluator(\"question-answering\")\n",
       ">>> data = load_dataset(\"squad_v2\", split=\"validation[:2]\")\n",
       ">>> results = task_evaluator.compute(\n",
       ">>>     model_or_pipeline=\"mrm8488/bert-tiny-finetuned-squadv2\",\n",
       ">>>     data=data,\n",
       ">>>     metric=\"squad_v2\",\n",
       ">>>     squad_v2_format=True,\n",
       ">>> )\n",
       "```\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/envs/genai-gcp/lib/python3.10/site-packages/evaluate/evaluator/question_answering.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_evaluator.compute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9bf6e8-c5bf-4294-9356-164e19767b2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of T5ForQuestionAnswering were not initialized from the model checkpoint at google/flan-t5-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, pipeline\n",
    "\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "tokenizer=T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.cls_token = tokenizer.pad_token\n",
    "\n",
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=MODEL_NAME,\n",
    "    tokenizer=tokenizer,\n",
    "    data=eval_dataset,\n",
    "    id_column='id',\n",
    "    question_column='context',\n",
    "    label_column='answer',\n",
    "    squad_v2_format=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6192804-6e40-4b2d-b4f2-c654c7d99ced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_results"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "genai-gcp",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kernelspec": {
   "display_name": "genai-gcp (Local)",
   "language": "python",
   "name": "genai-gcp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
