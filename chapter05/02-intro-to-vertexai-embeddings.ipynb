{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db6f41-d1ef-476a-a231-ef71c1c69966",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput\n",
    "\n",
    "PROJECT_ID = \"rthallam-demo-project\"\n",
    "REGION = \"us-central1\"\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
    "embedding = embedding_model.get_embeddings([\"The cat slept soundly on the soft rug.\"])\n",
    "\n",
    "vector = embedding[0].values\n",
    "print(f\"Length = {len(vector)}\")\n",
    "print(vector[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f70b7a-e4ef-4911-9c79-ce3e111ba35f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "reviews = [\n",
    "    \"\"\"I had to compare two versions of Hamlet for my Shakespeare class and \\\n",
    "unfortunately I picked this version. Everything from the acting (the actors \\\n",
    "deliver most of their lines directly to the camera) to the camera shots (all \\\n",
    "medium or close up shots...no scenery shots and very little back ground in the \\\n",
    "shots) were absolutely terrible. I watched this over my spring break and it is \\\n",
    "very safe to say that I feel that I was gypped out of 114 minutes of my \\\n",
    "vacation. Not recommended by any stretch of the imagination.\n",
    "Sentiment of the message: negative\"\"\",\n",
    "\n",
    "\"\"\"Something surprised me about this movie - it was actually original. It was not \\\n",
    "the same old recycled crap that comes out of Hollywood every month. I saw this \\\n",
    "movie on video because I did not even know about it before I saw it at my \\\n",
    "local video store. If you see this movie available - rent it - you will not \\\n",
    "regret it.\n",
    "Sentiment of the message: positive\"\"\",\n",
    "\n",
    "\"\"\"My family has watched Arthur Bach stumble and stammer since the movie first \\\n",
    "came out. We have most lines memorized. I watched it two weeks ago and still \\\n",
    "get tickled at the simple humor and view-at-life that Dudley Moore portrays. \\\n",
    "Liza Minelli did a wonderful job as the side kick - though I\\'m not her \\\n",
    "biggest fan. This movie makes me just enjoy watching movies. My favorite scene \\\n",
    "is when Arthur is visiting his fiancée\\'s house. His conversation with the \\\n",
    "butler and Susan\\'s father is side-spitting. The line from the butler, \\\n",
    "\"Would you care to wait in the Library\" followed by Arthur\\'s reply, \\\n",
    "\"Yes I would, the bathroom is out of the question\", is my NEWMAIL \\\n",
    "notification on my computer.\n",
    "Sentiment of the message: positive\"\"\",\n",
    "\n",
    "\"\"\"This Charles outing is decent but this is a pretty low-key performance. Marlon \\\n",
    "Brando stands out. There\\'s a subplot with Mira Sorvino and Donald Sutherland \\\n",
    "that forgets to develop and it hurts the film a little. I\\'m still trying to \\\n",
    "figure out why Charlie want to change his name.\n",
    "Sentiment of the message: negative\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2ce28b-cd6d-45c8-9b7f-7b6c054afb85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
    "input_reviews = [TextEmbeddingInput(task_type=\"CLASSIFICATION\", text=review) for review in reviews]\n",
    "embedding_reviews = embedding_model.get_embeddings(texts=input_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed76b20-213a-4506-ace8-458146123488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "embedding_reviews = [np.array(e.values) for e in embedding_reviews]\n",
    "dict_reviews = dict(reviews=reviews, embeddings=embedding_reviews)\n",
    "df_reviews = pd.DataFrame.from_dict(dict_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22495a6f-1a57-4a5e-9ba4-fcbb263f4513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2233d4e-5f09-4431-8782-c261e03d5ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dc9a2bca-bcea-4b7b-b3fa-a4b7edfa8388",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_similar_reviews(review: str, vector_store: pd.DataFrame, sort_index_value: int=2):\n",
    "    review_input = [TextEmbeddingInput(task_type=\"CLASSIFICATION\", text=review)]\n",
    "    review_vector = embedding_model.get_embeddings(texts=review_input)\n",
    "    vector_store[\"dot_product\"] = vector_store[\"embeddings\"].apply(lambda row: np.dot(row, review_vector))\n",
    "    top_matched = vector_store.sort_values(by=\"dot_product\", ascending=False)[:sort_index_value].index\n",
    "    top_matched_df = vector_store.loc[top_matched, [\"reviews\"]]\n",
    "    context = \"\\n\".join(top_matched_df[\"reviews\"].values)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e923e5ff-7b21-4107-b371-83594717f545",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (768,) and (1,) not aligned: 768 (dim 0) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m review \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mTweet: The Pixel 7 Pro, is too big to fit in my jeans pocket, so I bought \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mnew jeans.\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124mSentiment of the message: \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mget_similar_reviews\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_reviews\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m, in \u001b[0;36mget_similar_reviews\u001b[0;34m(review, vector_store, sort_index_value)\u001b[0m\n\u001b[1;32m      2\u001b[0m review_input \u001b[38;5;241m=\u001b[39m [TextEmbeddingInput(task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLASSIFICATION\u001b[39m\u001b[38;5;124m\"\u001b[39m, text\u001b[38;5;241m=\u001b[39mreview)]\n\u001b[1;32m      3\u001b[0m review_vector \u001b[38;5;241m=\u001b[39m embedding_model\u001b[38;5;241m.\u001b[39mget_embeddings(texts\u001b[38;5;241m=\u001b[39mreview_input)\n\u001b[0;32m----> 4\u001b[0m vector_store[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdot_product\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mvector_store\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43membeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreview_vector\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m top_matched \u001b[38;5;241m=\u001b[39m vector_store\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdot_product\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[:sort_index_value]\u001b[38;5;241m.\u001b[39mindex\n\u001b[1;32m      6\u001b[0m top_matched_df \u001b[38;5;241m=\u001b[39m vector_store\u001b[38;5;241m.\u001b[39mloc[top_matched, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviews\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[0;32m/opt/conda/envs/genai-gcp/lib/python3.10/site-packages/pandas/core/series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4800\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4918\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4920\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4922\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 4924\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/genai-gcp/lib/python3.10/site-packages/pandas/core/apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[1;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[0;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/genai-gcp/lib/python3.10/site-packages/pandas/core/apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[1;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[1;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1508\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[1;32m   1509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/conda/envs/genai-gcp/lib/python3.10/site-packages/pandas/core/base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[0;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/genai-gcp/lib/python3.10/site-packages/pandas/core/algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[0;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[1;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[1;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[1;32m   1747\u001b[0m     )\n",
      "File \u001b[0;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m, in \u001b[0;36mget_similar_reviews.<locals>.<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m      2\u001b[0m review_input \u001b[38;5;241m=\u001b[39m [TextEmbeddingInput(task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLASSIFICATION\u001b[39m\u001b[38;5;124m\"\u001b[39m, text\u001b[38;5;241m=\u001b[39mreview)]\n\u001b[1;32m      3\u001b[0m review_vector \u001b[38;5;241m=\u001b[39m embedding_model\u001b[38;5;241m.\u001b[39mget_embeddings(texts\u001b[38;5;241m=\u001b[39mreview_input)\n\u001b[0;32m----> 4\u001b[0m vector_store[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdot_product\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m vector_store[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreview_vector\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      5\u001b[0m top_matched \u001b[38;5;241m=\u001b[39m vector_store\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdot_product\u001b[39m\u001b[38;5;124m\"\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[:sort_index_value]\u001b[38;5;241m.\u001b[39mindex\n\u001b[1;32m      6\u001b[0m top_matched_df \u001b[38;5;241m=\u001b[39m vector_store\u001b[38;5;241m.\u001b[39mloc[top_matched, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreviews\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (768,) and (1,) not aligned: 768 (dim 0) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "review = \"\"\"Tweet: The Pixel 7 Pro, is too big to fit in my jeans pocket, so I bought \\\n",
    "new jeans.\n",
    "Sentiment of the message: \"\"\"\n",
    "\n",
    "get_similar_reviews(review, df_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4664e0-3e22-4495-a10c-b19c7d319858",
   "metadata": {},
   "source": [
    "# Multilingual embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c93f3fb9-702c-43f8-b5f9-32dd0cc5512e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length = 768\n",
      "[0.04212726652622223, 0.0060269092209637165, -0.041167981922626495, 0.033627621829509735, -0.013369345106184483]\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput\n",
    "\n",
    "PROJECT_ID = \"rthallam-demo-project\"\n",
    "REGION = \"us-central1\"\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "embedding_model = TextEmbeddingModel.from_pretrained(\"text-multilingual-embedding-002\")\n",
    "embedding = embedding_model.get_embeddings([\"休士頓哪裡有正宗的葡式蛋撻和港式麵包\"])\n",
    "\n",
    "vector = embedding[0].values\n",
    "print(f\"Length = {len(vector)}\")\n",
    "print(vector[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc12223-af06-4edb-a11a-d06767eb63eb",
   "metadata": {},
   "source": [
    "# Multimodal embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d94cbc8c-1b6b-4672-8eaf-82e1e9be9002",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mVMImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_bytes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcs_uri\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m      Image.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Creates an `Image` object.\n",
       "\n",
       "Args:\n",
       "    image_bytes: Image file bytes. Image can be in PNG or JPEG format.\n",
       "    gcs_uri: Image URI in Google Cloud Storage.\n",
       "\u001b[0;31mFile:\u001b[0m           /opt/conda/envs/genai-gcp/lib/python3.10/site-packages/vertexai/vision_models/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     GeneratedImage"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "VMImage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6a5eabec-81a0-4699-8a14-ab7e1c4e077e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mmm_embedding_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvertexai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvideo\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvertexai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcontextual_text\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdimension\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvideo_segment_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvertexai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVideoSegmentConfig\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'MultiModalEmbeddingResponse'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Gets embedding vectors from the provided image.\n",
       "\n",
       "Args:\n",
       "    image (Image): Optional. The image to generate embeddings for. One of\n",
       "      `image`, `video`, or `contextual_text` is required.\n",
       "    video (Video): Optional. The video to generate embeddings for. One of\n",
       "      `image`, `video` or `contextual_text` is required.\n",
       "    contextual_text (str): Optional. Contextual text for your input image or video.\n",
       "      If provided, the model will also generate an embedding vector for the\n",
       "      provided contextual text. The returned image and text embedding\n",
       "      vectors are in the same semantic space with the same dimensionality,\n",
       "      and the vectors can be used interchangeably for use cases like\n",
       "      searching image by text or searching text by image. One of `image`, `video` or\n",
       "      `contextual_text` is required.\n",
       "    dimension (int): Optional. The number of embedding dimensions. Lower\n",
       "      values offer decreased latency when using these embeddings for\n",
       "      subsequent tasks, while higher values offer better accuracy.\n",
       "      Available values: `128`, `256`, `512`, and `1408` (default).\n",
       "    video_segment_config (VideoSegmentConfig): Optional. The specific\n",
       "      video segments (in seconds) the embeddings are generated for.\n",
       "\n",
       "Returns:\n",
       "    MultiModalEmbeddingResponse:\n",
       "        The image and text embedding vectors.\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/envs/genai-gcp/lib/python3.10/site-packages/vertexai/vision_models/_vision_models.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm_embedding_model.get_embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "06c73681-9b62-417e-b09b-46c7dc9cc705",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of embedding:  256\n",
      "First five values are:  [0.170544401, 0.00523724314, 0.0162872504, -0.00227135723, 0.0331991129]\n"
     ]
    }
   ],
   "source": [
    "import vertexai\n",
    "from vertexai.vision_models import (\n",
    "    Image as VMImage,\n",
    "    MultiModalEmbeddingModel,\n",
    "    MultiModalEmbeddingResponse,\n",
    "    Video as VMVideo,\n",
    "    VideoSegmentConfig,\n",
    ")\n",
    "\n",
    "PROJECT_ID = \"rthallam-demo-project\"\n",
    "REGION = \"us-central1\"\n",
    "vertexai.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "mm_embedding_model = MultiModalEmbeddingModel.from_pretrained(\"multimodalembedding\")\n",
    "\n",
    "# Image embeddings with default 1408 dimension\n",
    "image_path = \"gs://github-repo/embeddings/getting_started_embeddings/gms_images/GGOEACBA104999.jpg\"\n",
    "image_path = image_path.replace(\"gs://\", \"https://storage.googleapis.com/\").replace(\" \", \"%20\")\n",
    "image = VMImage.load_from_file(image_path)\n",
    "dimension = 256\n",
    "image_embedding = mm_embedding_model.get_embeddings(image=image, dimension=dimension).image_embedding\n",
    "\n",
    "print(\"length of embedding: \", len(image_embedding))\n",
    "print(\"First five values are: \", image_embedding[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a14520f6-9b70-440b-b74b-2fd6e08f8641",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_bytes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mimage_bytes\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_bytes' is not defined"
     ]
    }
   ],
   "source": [
    "image_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6d6d4d15-8450-481e-86e9-429c3ef0d3b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def read_image_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download image: {response.status_code}\")\n",
    "\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/1/19/Solar_System_true_color.jpg\"\n",
    "image_bytes = read_image_from_url(image_url)\n",
    "# image = VMImage.load_from_file(image_bytes)\n",
    "image_embedding = mm_embedding_model.get_embeddings(image=image, dimension=dimension).image_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "95b065ef-1f5b-46b9-87d7-e7da64953670",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(image_embedding)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "genai-gcp",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kernelspec": {
   "display_name": "genai-gcp (Local)",
   "language": "python",
   "name": "genai-gcp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
